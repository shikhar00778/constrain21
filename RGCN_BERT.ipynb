{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RGCN Hindi Constraint.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb8925552f6649fe9c763aa2c49841a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f88e560880eb4d65b01f8d8572b52955",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f67a0f88a01849a7aac7e4a854f67fd7",
              "IPY_MODEL_497a8799fa024ee692fc5aee49f35760"
            ]
          }
        },
        "f88e560880eb4d65b01f8d8572b52955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f67a0f88a01849a7aac7e4a854f67fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b83cdb2e77f64321a0742a40e3874bc3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 434,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 434,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb306307719247eea5ec01ece7dd8ea3"
          }
        },
        "497a8799fa024ee692fc5aee49f35760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0192bc93483c45ad801774819a32bbc0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 434/434 [00:00&lt;00:00, 2.65kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ee9980ca75f4700aeb8c83ad1bcc841"
          }
        },
        "b83cdb2e77f64321a0742a40e3874bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb306307719247eea5ec01ece7dd8ea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0192bc93483c45ad801774819a32bbc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ee9980ca75f4700aeb8c83ad1bcc841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df21d73003a14af0ba8268f918d08ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13215569f73943c6a9976afd95f52a95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_47fcdba3ae684673b441f6d40b5717b3",
              "IPY_MODEL_a1e1dc725c1648eda52afa25ace3bbb4"
            ]
          }
        },
        "13215569f73943c6a9976afd95f52a95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47fcdba3ae684673b441f6d40b5717b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9650064e46fb4debbd59e7f2b9501b17",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_057bfeabfe1a47d9bd6c0f33e670c842"
          }
        },
        "a1e1dc725c1648eda52afa25ace3bbb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0136d1c085724ecebb8bf9d3798c6db7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.06MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ad098a4af8d46e299794619557caeb8"
          }
        },
        "9650064e46fb4debbd59e7f2b9501b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "057bfeabfe1a47d9bd6c0f33e670c842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0136d1c085724ecebb8bf9d3798c6db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ad098a4af8d46e299794619557caeb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7896e84ff55a41819c1f12f728e982f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bdb5e641051c47388371ac8b8254dc71",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_03bb85e0535c4479a358ca5719d8081f",
              "IPY_MODEL_8b49a53606e041398e162b562eb7f9ff"
            ]
          }
        },
        "bdb5e641051c47388371ac8b8254dc71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03bb85e0535c4479a358ca5719d8081f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78ccc66b319440beb9507d53cfd2fc5a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1344997306,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1344997306,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_551636b3f3f04a3fb40aa37c78f1a387"
          }
        },
        "8b49a53606e041398e162b562eb7f9ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24124ae0ca1041b1b7addaffb7f61841",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:19&lt;00:00, 67.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b79398fec71414d9570b7fecf5347ba"
          }
        },
        "78ccc66b319440beb9507d53cfd2fc5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "551636b3f3f04a3fb40aa37c78f1a387": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24124ae0ca1041b1b7addaffb7f61841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b79398fec71414d9570b7fecf5347ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY_7EzHvNrUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9518f6d1-8433-477d-871a-b61059cd30b3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3vTXSNSsC_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9bb980-5202-4a5d-e939-d7ccf0a7ea5e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 22 20:17:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1dtAgzrpZxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6b6aa1-d953-4034-9a58-18fd31375d6f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W9M4jSkjQ0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e75686-30b9-4258-c13f-cd703814d791"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180945 sha256=776a5cb74968eb3ee23ebdcd856beab5b6d734a121727ff6ac64e1f91921ecde\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-min15de5/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoLmxpSHAQym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52697edb-3dcf-4007-b76c-058a54673955"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50sb3JL_N8Ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0700d1-c1eb-4970-d94c-ce7f498e1f7b"
      },
      "source": [
        "!pip install dgl-cu101"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl-cu101\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/08/ea2d56e85eba1c22a14fa0f9b3c4ca8b43bf07de34e454d4e23632b376ea/dgl_cu101-0.5.3-cp36-cp36m-manylinux1_x86_64.whl (25.0MB)\n",
            "\u001b[K     |████████████████████████████████| 25.0MB 138kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl-cu101) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl-cu101) (2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl-cu101) (1.19.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from dgl-cu101) (2.23.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl-cu101) (4.4.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl-cu101) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl-cu101) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl-cu101) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl-cu101) (1.24.3)\n",
            "Installing collected packages: dgl-cu101\n",
            "Successfully installed dgl-cu101-0.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiWK77Wvgqwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39bf96a-fd5f-4fbe-f7c4-b89d01063ebb"
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip install emot --upgrade\n",
        "!pip install transformers==3.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 24.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 29.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 32.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 23.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 19.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 21.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 20.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 20.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 20.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 20.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.19.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/c1/5c2b2259dd4a149f873f1ab9b4c5ef106c828a4abc7230c9452be8c27493/boto3-1.16.41-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.41\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/69/eecc498592e2ee9a300037881b38637358dbddd5211d2af061c8b177abe4/botocore-1.19.41-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 41.2MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.41->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.41->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.41 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.41 botocore-1.19.41 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n",
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n",
            "Collecting transformers==3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/34/fb092588df61bf33f113ade030d1cbe74fb73a0353648f8dd938a223dce7/transformers-3.5.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.19.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.0.12)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (51.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=01dd152e242940954bb7c8f68606c7eae9dff2b426dff10134d3b8d896b632b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHUnD2rZh-AI"
      },
      "source": [
        "**Generate Pickle Files for feeding as embedding in Graphs. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7plbrz9gjU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5196d6-0435-4599-9a08-c9e03104391d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import dgl\n",
        "from dgl import DGLGraph\n",
        "from dgl.data import MiniGCDataset\n",
        "import dgl.function as fn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "#from transformers import BertTokenizer,BertModel\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertModel\n",
        "\n",
        "\n",
        "import spacy\n",
        "import pickle\n",
        "import collections\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "import os\n",
        "import re\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pukJ94ZoLk3"
      },
      "source": [
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOoXNs5ppAbO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "eb8925552f6649fe9c763aa2c49841a0",
            "f88e560880eb4d65b01f8d8572b52955",
            "f67a0f88a01849a7aac7e4a854f67fd7",
            "497a8799fa024ee692fc5aee49f35760",
            "b83cdb2e77f64321a0742a40e3874bc3",
            "cb306307719247eea5ec01ece7dd8ea3",
            "0192bc93483c45ad801774819a32bbc0",
            "3ee9980ca75f4700aeb8c83ad1bcc841",
            "df21d73003a14af0ba8268f918d08ad7",
            "13215569f73943c6a9976afd95f52a95",
            "47fcdba3ae684673b441f6d40b5717b3",
            "a1e1dc725c1648eda52afa25ace3bbb4",
            "9650064e46fb4debbd59e7f2b9501b17",
            "057bfeabfe1a47d9bd6c0f33e670c842",
            "0136d1c085724ecebb8bf9d3798c6db7",
            "3ad098a4af8d46e299794619557caeb8"
          ]
        },
        "outputId": "f4634859-64f8-4942-a3a5-cc24849ab3d8"
      },
      "source": [
        "BERT_MODEL = 'bert-large-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL, never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb8925552f6649fe9c763aa2c49841a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df21d73003a14af0ba8268f918d08ad7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t0nhShvhvjP"
      },
      "source": [
        "#df_train = pd.read_csv('drive/MyDrive/Constraint_Hindi/Constraint_Hindi_Train.csv')\n",
        "#df_valid = pd.read_csv('drive/MyDrive/Constraint_Hindi/Constraint_Hindi_Valid.csv')\n",
        "\n",
        "df_train = pd.read_csv('hindi_train_translated.csv')\n",
        "df_valid = pd.read_csv('hindi_val_translated.csv')\n",
        "df_test = pd.read_csv('hindi_test_translated.csv')\n",
        "\n",
        "df_train_hindi = pd.read_csv('Constraint_Hindi_Train.csv')\n",
        "df_valid_hindi = pd.read_csv('Constraint_Hindi_Valid.csv')\n",
        "df_test_hindi = pd.read_csv('Constraint_Hindi_Test.csv')\n",
        "\n",
        "df_train['Unique ID'] = df_train_hindi['Unique ID']\n",
        "df_valid['Unique ID'] = df_valid_hindi['Unique ID']\n",
        "df_test['Unique ID'] = df_test_hindi['Unique ID']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWpZ9QGdokEu"
      },
      "source": [
        "MAX_LEN = 100 #max([len(i.split(\" \")) for i in list(df_train['Post'])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrWtrjyrMuaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484aa8e0-e58e-4307-acf9-67c5c136cc6d"
      },
      "source": [
        "tokens_lst = []\n",
        "for _, row in df_train.iterrows():\n",
        "    \n",
        "    text = row['Post'].split(' ')\n",
        "    #print(text)\n",
        "    text = ' '.join(text[:MAX_LEN])\n",
        "    #print(text)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens_lst.append(tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
        "\n",
        "max_len = max((len(x) for x in tokens_lst))\n",
        "print(max_len)\n",
        "tokens = np.zeros((len(tokens_lst), max_len), dtype=np.int64)\n",
        "for i, row in enumerate(tokens_lst):\n",
        "    row = np.array(row[:max_len])\n",
        "    tokens[i, :len(row)] = row\n",
        "\n",
        "# All sentenses\n",
        "token_tensor = torch.from_numpy(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGhNSu9hbMNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3dbb02b-35b1-45e4-b3bd-cfac492e01ef"
      },
      "source": [
        "tokens_lst_valid = []\n",
        "for _, row in df_valid.iterrows():\n",
        "    \n",
        "    text = row['Post'].split(' ')\n",
        "    #print(text)\n",
        "    text = ' '.join(text[:MAX_LEN])\n",
        "    #print(text)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens_lst_valid.append(tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
        "\n",
        "max_len_valid = max((len(x) for x in tokens_lst_valid))\n",
        "print(max_len_valid)\n",
        "tokens = np.zeros((len(tokens_lst_valid), max_len_valid), dtype=np.int64)\n",
        "for i, row in enumerate(tokens_lst_valid):\n",
        "    row = np.array(row[:max_len_valid])\n",
        "    tokens[i, :len(row)] = row\n",
        "\n",
        "# All sentenses\n",
        "token_tensor_valid = torch.from_numpy(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3A7Rqk6ERW6",
        "outputId": "ff843ee4-a2d9-4a44-fe4a-162523dc16d8"
      },
      "source": [
        "tokens_lst_test = []\n",
        "for _, row in df_test.iterrows():\n",
        "    \n",
        "    text = row['Post'].split(' ')\n",
        "    #print(text)\n",
        "    text = ' '.join(text[:MAX_LEN])\n",
        "    #print(text)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens_lst_test.append(tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
        "\n",
        "max_len_valid = max((len(x) for x in tokens_lst_test))\n",
        "print(max_len_valid)\n",
        "tokens = np.zeros((len(tokens_lst_test), max_len_valid), dtype=np.int64)\n",
        "for i, row in enumerate(tokens_lst_test):\n",
        "    row = np.array(row[:max_len_valid])\n",
        "    tokens[i, :len(row)] = row\n",
        "\n",
        "# All sentenses\n",
        "token_tensor_test = torch.from_numpy(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIomfMj_J_qJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7896e84ff55a41819c1f12f728e982f9",
            "bdb5e641051c47388371ac8b8254dc71",
            "03bb85e0535c4479a358ca5719d8081f",
            "8b49a53606e041398e162b562eb7f9ff",
            "78ccc66b319440beb9507d53cfd2fc5a",
            "551636b3f3f04a3fb40aa37c78f1a387",
            "24124ae0ca1041b1b7addaffb7f61841",
            "1b79398fec71414d9570b7fecf5347ba"
          ]
        },
        "outputId": "f61e1db5-483b-4b6c-ea71-21878865d8be"
      },
      "source": [
        "#xlm_roberta = XLMRobertaModel.from_pretrained(XLM_ROBERTA_MODEL)\n",
        "bert_model = transformers.AutoModel.from_pretrained(BERT_MODEL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7896e84ff55a41819c1f12f728e982f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgGqMsrpF2jd"
      },
      "source": [
        "#xlm_roberta.cuda()\n",
        "bert_model.cuda()\n",
        "token_tensor = token_tensor.cuda()\n",
        "token_tensor_valid = token_tensor_valid.cuda()\n",
        "token_tensor_test = token_tensor_test.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp6QDzhVFJ0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401c66e4-1e4d-4df0-9e7c-12ee6f5eb505"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "bert_outputs = []\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(range(len(token_tensor))):\n",
        "        \n",
        "        bert_output, _ =  bert_model(\n",
        "                    token_tensor[i].unsqueeze(0), \n",
        "                    attention_mask=(token_tensor[i].unsqueeze(0) > 0).long(), \n",
        "                    token_type_ids=None, \n",
        "                    #output_all_encoded_layers=False) \n",
        "        )\n",
        "\n",
        "        bert_outputs.append(bert_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5728/5728 [04:05<00:00, 23.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrSAcWv-bfgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bb99af-0942-4a34-e9e3-f2356c1902ff"
      },
      "source": [
        "\n",
        "bert_outputs_valid = []\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(range(len(token_tensor_valid))):\n",
        "        \n",
        "        bert_output, _ =  bert_model(\n",
        "                    token_tensor_valid[i].unsqueeze(0), \n",
        "                    attention_mask=(token_tensor_valid[i].unsqueeze(0) > 0).long(), \n",
        "                    token_type_ids=None, \n",
        "                    #output_all_encoded_layers=False) \n",
        "        )\n",
        "\n",
        "        bert_outputs_valid.append(bert_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 811/811 [00:38<00:00, 20.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MICtYb72EI7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a852275-70bc-4361-ffc4-95c327b8663f"
      },
      "source": [
        "\n",
        "bert_outputs_test = []\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(range(len(token_tensor_test)) , position = 0):\n",
        "        \n",
        "        bert_output, _ =  bert_model(\n",
        "                    token_tensor_test[i].unsqueeze(0), \n",
        "                    attention_mask=(token_tensor_test[i].unsqueeze(0) > 0).long(), \n",
        "                    token_type_ids=None, \n",
        "                    #output_all_encoded_layers=False) \n",
        "        )\n",
        "\n",
        "        bert_outputs_test.append(bert_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1653/1653 [01:18<00:00, 21.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWs5MwFwFLF2"
      },
      "source": [
        "pickle.dump(tokens_lst, open('token_lst_wto_padding.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkiUyq2yfMdG"
      },
      "source": [
        "torch.save(bert_outputs , 'bert_outputs.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUnX_0GAb1fo"
      },
      "source": [
        "pickle.dump(tokens_lst_valid, open('token_lst_wto_padding_valid.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUdC2-gHiB1a"
      },
      "source": [
        "pickle.dump(bert_outputs_valid, open('bert_outputs_valid.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhJnB6JAEv1_"
      },
      "source": [
        "pickle.dump(tokens_lst_test, open('token_lst_wto_padding_test.pkl', \"wb\"))\n",
        "pickle.dump(bert_outputs_test, open('bert_outputs_test.pkl', \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEU5L337iI1Y"
      },
      "source": [
        "**Prepare DGL graphs for the RGCN model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbwenx_sLad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f1a9be-73f6-4380-eaeb-73510e2b1cbb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import dgl\n",
        "from dgl import DGLGraph\n",
        "from dgl.data import MiniGCDataset\n",
        "import dgl.function as fn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "import spacy\n",
        "import pickle\n",
        "import collections\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import re\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "from dgl.data.utils import save_graphs,load_graphs\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnSliAphJ8FI"
      },
      "source": [
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjEKXOFuNYpX"
      },
      "source": [
        "parser = spacy.load('en_core_web_lg')\n",
        "parser.tokenizer.add_special_case(\"[UNK]\", [{\"ORTH\": \"[UNK]\"}])\n",
        "parser.tokenizer.add_special_case(\"[SEP]\", [{\"ORTH\": \"[SEP]\"}])\n",
        "parser.tokenizer.add_special_case(\"[PAD]\", [{\"ORTH\": \"[PAD]\"}])\n",
        "parser.tokenizer.add_special_case(\"[CLS]\", [{\"ORTH\": \"[CLS]\"}])\n",
        "parser.tokenizer.add_special_case(\"[MASK]\", [{\"ORTH\": \"[MASK]\"}])\n",
        "\n",
        "BERT_MODEL = 'bert-large-uncased'\n",
        "#BERT_MODEL = 'xlm-roberta-large'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL, never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-7uICVvf0Oe"
      },
      "source": [
        "token_lst = pickle.load(open('token_lst_wto_padding.pkl', \"rb\")) # tokens of every sentence without padding\n",
        "bert_outputs_lst = torch.load('bert_outputs.pt') # list of outputs of bert for every sentence\n",
        "\n",
        "token_lst_valid = pickle.load(open('token_lst_wto_padding_valid.pkl', \"rb\")) # tokens of every sentence without padding\n",
        "bert_outputs_lst_valid = pickle.load(open('bert_outputs_valid.pkl', \"rb\")) # list of outputs of bert for every sentence\n",
        "\n",
        "token_lst_test = pickle.load(open('token_lst_wto_padding_test.pkl', \"rb\")) # tokens of every sentence without padding\n",
        "bert_outputs_lst_test = pickle.load(open('bert_outputs_test.pkl', \"rb\")) # list of outputs of bert for every sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFTQIZURgGx6",
        "outputId": "5c316a92-bb8f-4740-ae69-26743828fa1b"
      },
      "source": [
        "bert_outputs_lst[0].shape , bert_outputs_lst_valid[0].shape , bert_outputs_lst_test[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 157, 1024]),\n",
              " torch.Size([1, 177, 1024]),\n",
              " torch.Size([1, 176, 1024]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1gpa27rNcP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179b4ad9-d580-4f0b-c566-90170d2c92aa"
      },
      "source": [
        "def transfer_n_e(nodes, edges):\n",
        "\n",
        "    num_nodes = len(nodes)\n",
        "    new_edges = []\n",
        "    for e1, e2 in edges:\n",
        "        new_edges.append( [nodes[e1], nodes[e2]] ) \n",
        "    return num_nodes, new_edges\n",
        "\n",
        "all_graphs = []\n",
        "gcn_offsets = []\n",
        "for i, sent_token in tqdm.tqdm(enumerate(token_lst),position=0):\n",
        "    \n",
        "    sent_token = token_lst[i]\n",
        "    sent = ' '.join([re.sub(\"[#]\",\"\",token)   for token in tokenizer.convert_ids_to_tokens(sent_token[1:-1])])\n",
        "\n",
        "    #sent = ' '.join([re.sub(\"[▁]\",\"\",token)   for token in tokenizer.convert_ids_to_tokens(sent_token[1:-1])])\n",
        "    #print(len(token_lst[i]) , tokenizer.convert_ids_to_tokens(sent_token[1:-1]))\n",
        "    #print(sent)\n",
        "    #print(len(sent.split(' ')))\n",
        "\n",
        "    doc = parser(sent)\n",
        "    parse_rst = doc.to_json()\n",
        "\n",
        "    nodes = collections.OrderedDict()\n",
        "    edges = []\n",
        "    edge_type = []\n",
        "    for i_word, word in enumerate(parse_rst['tokens']):\n",
        "        \n",
        "        if i_word not in nodes:\n",
        "            nodes[i_word] = len(nodes) \n",
        "            edges.append( [i_word, i_word] )\n",
        "            edge_type.append(0)\n",
        "        if word['head'] not in nodes:\n",
        "            nodes[word['head']] = len(nodes) \n",
        "            edges.append( [word['head'], word['head']] )\n",
        "            edge_type.append(0)\n",
        "\n",
        "        if word['dep'] != 'ROOT':\n",
        "                edges.append( [word['head'], word['id']] )\n",
        "                edge_type.append(1)\n",
        "                edges.append( [word['id'], word['head']] )\n",
        "                edge_type.append(2)\n",
        "\n",
        "    num_nodes, tran_edges = transfer_n_e(nodes, edges)\n",
        "    \n",
        "    #print(num_nodes , bert_outputs_lst[i].shape)\n",
        "\n",
        "    G = dgl.DGLGraph().to('cuda')\n",
        "    G.add_nodes(num_nodes)\n",
        "    G.add_edges(list(zip(*tran_edges))[0],list(zip(*tran_edges))[1]) \n",
        "\n",
        "    for i_word, word in enumerate(parse_rst['tokens']):\n",
        "        G.nodes[ [ nodes[i_word] ]].data['h'] = bert_outputs_lst[i][0][i_word + 1].unsqueeze(0).cuda()\n",
        "        G.nodes[ [ nodes[word['head']] ]].data['h'] = bert_outputs_lst[i][0][word['head'] + 1].unsqueeze(0).cuda()\n",
        "\n",
        "    edge_norm = []\n",
        "    for e1, e2 in tran_edges:\n",
        "        if e1 == e2:\n",
        "            edge_norm.append(1)\n",
        "        else:\n",
        "            edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
        "\n",
        "\n",
        "    edge_type = torch.from_numpy(np.array(edge_type)).cuda()\n",
        "    edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float().cuda()\n",
        "\n",
        "    G.edata.update({'rel_type': edge_type,})\n",
        "    G.edata.update({'norm': edge_norm})\n",
        "    all_graphs.append(G)\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: DGLGraph.in_degree is deprecated. Please use DGLGraph.in_degrees\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "5728it [06:07, 15.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JgjaM1thVep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc081b3-3550-44bf-8b1f-9dd8ffda30a6"
      },
      "source": [
        "all_graphs_valid = []\n",
        "\n",
        "for i, sent_token in tqdm.tqdm(enumerate(token_lst_valid)):\n",
        "    \n",
        "    sent_token = token_lst_valid[i]\n",
        "\n",
        "    sent = ' '.join([re.sub(\"[#]\",\"\",token)   for token in tokenizer.convert_ids_to_tokens(sent_token[1:-1])])\n",
        "    #print(i,len(token_lst[i]) , tokenizer.convert_ids_to_tokens(sent_token[1:-1]))\n",
        "    #print(sent)\n",
        "    #print(len(sent.split(' ')))\n",
        "\n",
        "    doc = parser(sent)\n",
        "    parse_rst = doc.to_json()\n",
        "\n",
        "    nodes = collections.OrderedDict()\n",
        "    edges = []\n",
        "    edge_type = []\n",
        "    for i_word, word in enumerate(parse_rst['tokens']):\n",
        "        \n",
        "        if i_word not in nodes:\n",
        "            nodes[i_word] = len(nodes) \n",
        "            edges.append( [i_word, i_word] )\n",
        "            edge_type.append(0)\n",
        "        if word['head'] not in nodes:\n",
        "            nodes[word['head']] = len(nodes) \n",
        "            edges.append( [word['head'], word['head']] )\n",
        "            edge_type.append(0)\n",
        "\n",
        "        if word['dep'] != 'ROOT':\n",
        "                edges.append( [word['head'], word['id']] )\n",
        "                edge_type.append(1)\n",
        "                edges.append( [word['id'], word['head']] )\n",
        "                edge_type.append(2)\n",
        "\n",
        "    num_nodes, tran_edges = transfer_n_e(nodes, edges)\n",
        "    \n",
        "    #print(num_nodes , bert_outputs_lst[i].shape)\n",
        "\n",
        "    G = dgl.DGLGraph().to('cuda')\n",
        "    G.add_nodes(num_nodes)\n",
        "    G.add_edges(list(zip(*tran_edges))[0],list(zip(*tran_edges))[1]) \n",
        "    #if i >= 290:\n",
        "    #  print(i)\n",
        "    for i_word, word in enumerate(parse_rst['tokens']):\n",
        "        G.nodes[ [ nodes[i_word] ]].data['h'] = bert_outputs_lst_valid[i][0][i_word + 1].unsqueeze(0).cuda()\n",
        "        G.nodes[ [ nodes[word['head']] ]].data['h'] = bert_outputs_lst_valid[i][0][word['head'] + 1].unsqueeze(0).cuda()\n",
        "\n",
        "    edge_norm = []\n",
        "    for e1, e2 in tran_edges:\n",
        "        if e1 == e2:\n",
        "            edge_norm.append(1)\n",
        "        else:\n",
        "            edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
        "\n",
        "\n",
        "    edge_type = torch.from_numpy(np.array(edge_type)).cuda()\n",
        "    edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float().cuda()\n",
        "\n",
        "    G.edata.update({'rel_type': edge_type,})\n",
        "    G.edata.update({'norm': edge_norm})\n",
        "    all_graphs_valid.append(G)\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: DGLGraph.in_degree is deprecated. Please use DGLGraph.in_degrees\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "811it [00:52, 15.55it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMQDpsF8FI2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48e40fd-3f92-44c4-f0d4-6a0f85115d32"
      },
      "source": [
        "all_graphs_test = []\n",
        "\n",
        "for i, sent_token in tqdm.tqdm(enumerate(token_lst_test) , position = 0):\n",
        "    \n",
        "    sent_token = token_lst_test[i]\n",
        "\n",
        "    sent = ' '.join([re.sub(\"[#]\",\"\",token)   for token in tokenizer.convert_ids_to_tokens(sent_token[1:-1])])\n",
        "    #print(i,len(token_lst[i]) , tokenizer.convert_ids_to_tokens(sent_token[1:-1]))\n",
        "    #print(sent)\n",
        "    #print(len(sent.split(' ')))\n",
        "\n",
        "    doc = parser(sent)\n",
        "    parse_rst = doc.to_json()\n",
        "\n",
        "    nodes = collections.OrderedDict()\n",
        "    edges = []\n",
        "    edge_type = []\n",
        "    for i_word, word in enumerate(parse_rst['tokens']):\n",
        "        \n",
        "        if i_word not in nodes:\n",
        "            nodes[i_word] = len(nodes) \n",
        "            edges.append( [i_word, i_word] )\n",
        "            edge_type.append(0)\n",
        "        if word['head'] not in nodes:\n",
        "            nodes[word['head']] = len(nodes) \n",
        "            edges.append( [word['head'], word['head']] )\n",
        "            edge_type.append(0)\n",
        "\n",
        "        if word['dep'] != 'ROOT':\n",
        "                edges.append( [word['head'], word['id']] )\n",
        "                edge_type.append(1)\n",
        "                edges.append( [word['id'], word['head']] )\n",
        "                edge_type.append(2)\n",
        "\n",
        "    num_nodes, tran_edges = transfer_n_e(nodes, edges)\n",
        "    \n",
        "    #print(num_nodes , bert_outputs_lst[i].shape)\n",
        "\n",
        "    G = dgl.DGLGraph().to('cuda')\n",
        "    G.add_nodes(num_nodes)\n",
        "    G.add_edges(list(zip(*tran_edges))[0],list(zip(*tran_edges))[1]) \n",
        "    #if i >= 290:\n",
        "    #  print(i)\n",
        "    for i_word, word in enumerate(parse_rst['tokens']):\n",
        "        G.nodes[ [ nodes[i_word] ]].data['h'] = bert_outputs_lst_test[i][0][i_word + 1].unsqueeze(0).cuda()\n",
        "        G.nodes[ [ nodes[word['head']] ]].data['h'] = bert_outputs_lst_test[i][0][word['head'] + 1].unsqueeze(0).cuda()\n",
        "\n",
        "    edge_norm = []\n",
        "    for e1, e2 in tran_edges:\n",
        "        if e1 == e2:\n",
        "            edge_norm.append(1)\n",
        "        else:\n",
        "            edge_norm.append( 1 / (G.in_degree(e2) - 1 ) )\n",
        "\n",
        "\n",
        "    edge_type = torch.from_numpy(np.array(edge_type)).cuda()\n",
        "    edge_norm = torch.from_numpy(np.array(edge_norm)).unsqueeze(1).float().cuda()\n",
        "\n",
        "    G.edata.update({'rel_type': edge_type,})\n",
        "    G.edata.update({'norm': edge_norm})\n",
        "    all_graphs_test.append(G)\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: DGLGraph.in_degree is deprecated. Please use DGLGraph.in_degrees\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "1653it [01:42, 16.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlBXAgWHitGc"
      },
      "source": [
        "save_graphs(\"graph_data_valid.bin\", all_graphs_valid)\n",
        "save_graphs(\"graph_data.bin\", all_graphs)\n",
        "save_graphs(\"graph_data_test.bin\", all_graphs_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJSnpMz1i1gI"
      },
      "source": [
        "**Final Model Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH5Pb5z7jBAA",
        "outputId": "1ac3e70c-45cf-4e5a-8eb2-f28c99d89101"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import dgl\n",
        "from dgl import DGLGraph\n",
        "from dgl.data import MiniGCDataset\n",
        "import dgl.function as fn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "import spacy\n",
        "import pickle\n",
        "import collections\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import re\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "from dgl.data.utils import save_graphs,load_graphs\n",
        "from time import sleep"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3KPRQ6bjBpK"
      },
      "source": [
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q52ij1SHLyPg"
      },
      "source": [
        "token_lst = pickle.load(open('token_lst_wto_padding.pkl', \"rb\")) # tokens of every sentence without padding\n",
        "#bert_outputs_lst = torch.load('bert_outputs.pt') # list of outputs of bert for every sentence\n",
        "\n",
        "\n",
        "token_lst_valid = pickle.load(open('token_lst_wto_padding_valid.pkl', \"rb\")) # tokens of every sentence without padding\n",
        "#bert_outputs_lst_valid = pickle.load(open('bert_outputs_valid.pkl', \"rb\")) # list of outputs of bert for every sentence\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jO-aLblmZkv"
      },
      "source": [
        "all_graphs = load_graphs(\"graph_data.bin\")[0]\n",
        "all_graphs = [i.to('cuda') for i in all_graphs]\n",
        "all_graphs_valid = load_graphs(\"graph_data_valid.bin\")[0]\n",
        "all_graphs_valid = [i.to('cuda') for i in all_graphs_valid]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBhyet9irF6d"
      },
      "source": [
        "RGCN_output_size_1 = 1024\n",
        "RGCN_output_size_2 = 768\n",
        "RGCN_output_size_3 = 512\n",
        "RGCN_output_size_4 = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LF6npTJRT9t"
      },
      "source": [
        "#df_train = pd.read_csv('drive/MyDrive/Constraint_Hindi/Constraint_Hindi_Train.csv')\n",
        "#df_valid = pd.read_csv('drive/MyDrive/Constraint_Hindi/Constraint_Hindi_Valid.csv')\n",
        "df_train = pd.read_csv('hindi_train_translated.csv')\n",
        "df_valid = pd.read_csv('hindi_val_translated.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1946skxlI_vr"
      },
      "source": [
        "label = {}\n",
        "c = 0\n",
        "cats = list(set([j.strip() for i in df_train['Labels Set'] for j in i.split(',')]))\n",
        "cats.sort()\n",
        "for j in cats:\n",
        "  label[j] = c\n",
        "  c = c+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ESHR69mG-zA"
      },
      "source": [
        "#df_train_hindi = pd.read_csv('Augment_Train_Hindi.csv')\n",
        "df_train_hindi = pd.read_csv('Constraint_Hindi_Train.csv')\n",
        "df_valid_hindi = pd.read_csv('Constraint_Hindi_Valid.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQfBAT5mH9_2"
      },
      "source": [
        "def fix_labels(df):\n",
        "  final_label = []\n",
        "\n",
        "  for labels in list(df['Labels Set']):\n",
        "    labels_tensor = np.zeros(c , dtype=np.int64)\n",
        "    #print(labels , labels_tensor)\n",
        "    for i in labels.split(\",\"):\n",
        "      labels_tensor[label[i.strip()]] = 1\n",
        "    final_label.append(list(labels_tensor))\n",
        "\n",
        "  return final_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-yrBmEgNq0h"
      },
      "source": [
        "df_train_hindi['Labels'] = fix_labels(df_train_hindi)\n",
        "df_valid_hindi['Labels'] = fix_labels(df_valid_hindi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLHBhsdoOZup"
      },
      "source": [
        "new_df = df_train_hindi[['Post', 'Labels']].copy()\n",
        "new_df.columns = ['comment_text' , 'list']\n",
        "#new_df.head()\n",
        "\n",
        "new_df_valid = df_valid_hindi[['Post', 'Labels']].copy()\n",
        "new_df_valid.columns = ['comment_text' , 'list']\n",
        "#new_df_valid.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwSeIUSoOjsB"
      },
      "source": [
        "import transformers\n",
        "MAX_LEN = 128\n",
        "BERT_MODEL = 'bert-base-multilingual-cased'\n",
        "config = transformers.AutoConfig.from_pretrained(BERT_MODEL)\n",
        "mBERT_OUTPUT_SIZE = config.hidden_size\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(BERT_MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oT7JMxYPR6Y"
      },
      "source": [
        "class GPRDataset(Dataset):\n",
        "    def __init__(self, original_df, graphs , dataframe, tokenizer, max_len , datatype = 'train'):\n",
        "        \n",
        "        self.datatype = datatype\n",
        "        if self.datatype != 'test':\n",
        "          self.y = original_df['Labels Set']\n",
        "\n",
        "        self.graphs = graphs\n",
        "        #self.bert_embeddings = bert_embeddings\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.comment_text = dataframe.comment_text\n",
        "        #self.targets = self.data.list\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.datatype != 'test':\n",
        "          labels = self.y[idx].split(\",\")\n",
        "          labels_tensor = np.zeros(c , dtype=np.int64)\n",
        "          #print(labels , labels_tensor)\n",
        "          for i in labels:\n",
        "            #print(i.strip())\n",
        "            labels_tensor[label[i.strip()]] = 1\n",
        "\n",
        "        \n",
        "        comment_text = str(self.comment_text[idx])\n",
        "        comment_text = \" \".join(comment_text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        #return self.graphs[idx], self.bert_embeddings[idx] , labels_tensor , torch.tensor(ids, dtype=torch.long).cuda(),torch.tensor(mask, dtype=torch.long).cuda(), torch.tensor(token_type_ids, dtype=torch.long).cuda()\n",
        "        if self.datatype != 'test':\n",
        "          return self.graphs[idx], labels_tensor , torch.tensor(ids, dtype=torch.long).cuda(),torch.tensor(mask, dtype=torch.long).cuda(), torch.tensor(token_type_ids, dtype=torch.long).cuda()\n",
        "        else:\n",
        "          return self.graphs[idx], torch.tensor(ids, dtype=torch.long).cuda(),torch.tensor(mask, dtype=torch.long).cuda(), torch.tensor(token_type_ids, dtype=torch.long).cuda()\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrAXGBlBQZv8"
      },
      "source": [
        "def collate(samples):\n",
        "    \n",
        "    #graphs, bert_embeddings , labels , ids , mask , token_type_ids = map(list, zip(*samples))\n",
        "    graphs , labels , ids , mask , token_type_ids = map(list, zip(*samples))\n",
        "\n",
        "    batched_graph = dgl.batch(graphs)\n",
        "    labels = torch.FloatTensor(labels)\n",
        "    \n",
        "    #bert_embeddings = [] #torch.stack(bert_embeddings, dim=0).squeeze()\n",
        "\n",
        "    ids = torch.stack(ids, dim=0).squeeze()\n",
        "    mask = torch.stack(mask, dim=0).squeeze()\n",
        "    token_type_ids = torch.stack(token_type_ids, dim=0).squeeze()\n",
        "\n",
        "    #return batched_graph, bert_embeddings , labels , ids , mask , token_type_ids\n",
        "    return batched_graph , labels , ids , mask , token_type_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_IKGebANZRW"
      },
      "source": [
        "lr_value = 0.0001\n",
        "total_epoch = 10\n",
        "def adjust_learning_rate(optimizers, epoch):\n",
        "    # warm up\n",
        "    if epoch < 10:\n",
        "        lr_tmp = 0.00001\n",
        "    else:\n",
        "        lr_tmp = lr_value * pow((1 - 1.0 * epoch / 100), 0.9)\n",
        "    \n",
        "    if epoch > 36:\n",
        "        lr_tmp =  0.000015 * pow((1 - 1.0 * epoch / 100), 0.9)\n",
        "    \n",
        "    for optimizer in optimizers:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr_tmp\n",
        "\n",
        "    return lr_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjryAZy3vFU9"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import math\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "sigmoid_v = np.vectorize(sigmoid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdElTPmFL_Fn"
      },
      "source": [
        "class RGCNLayer(nn.Module):\n",
        "    def __init__(self, feat_size, num_rels, activation=None, gated = True):\n",
        "        \n",
        "        super(RGCNLayer, self).__init__()\n",
        "        self.feat_size = feat_size\n",
        "        self.num_rels = num_rels\n",
        "        self.activation = activation\n",
        "        self.gated = gated\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(self.num_rels, self.feat_size, RGCN_output_size_1))\n",
        "        # init trainable parameters\n",
        "        nn.init.xavier_uniform_(self.weight,gain=nn.init.calculate_gain('relu'))\n",
        "        \n",
        "        if self.gated:\n",
        "            self.gate_weight = nn.Parameter(torch.Tensor(self.num_rels, self.feat_size, 1))\n",
        "            nn.init.xavier_uniform_(self.gate_weight,gain=nn.init.calculate_gain('sigmoid'))\n",
        "        \n",
        "    def forward(self, g):\n",
        "        \n",
        "        weight = self.weight\n",
        "        gate_weight = self.gate_weight\n",
        "        \n",
        "        def message_func(edges):\n",
        "            w = weight[edges.data['rel_type']]\n",
        "            #print(edges.src['h'].shape)\n",
        "            #print(edges.src['h'])\n",
        "            msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
        "            msg = msg * edges.data['norm']\n",
        "            \n",
        "            if self.gated:\n",
        "                gate_w = gate_weight[edges.data['rel_type']]\n",
        "                gate = torch.bmm(edges.src['h'].unsqueeze(1), gate_w).squeeze().reshape(-1,1)\n",
        "                gate = torch.sigmoid(gate)\n",
        "                msg = msg * gate\n",
        "                \n",
        "            return {'msg': msg}\n",
        "    \n",
        "        def apply_func(nodes):\n",
        "            h = nodes.data['h']\n",
        "            h = self.activation(h)\n",
        "            return {'h': h}\n",
        "\n",
        "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GIJA9JTNRIQ"
      },
      "source": [
        "class RGCNModel(nn.Module):\n",
        "    def __init__(self, h_dim, num_rels, num_hidden_layers=1, gated = True):\n",
        "        super(RGCNModel, self).__init__()\n",
        "\n",
        "        self.h_dim = h_dim\n",
        "        self.num_rels = num_rels\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.gated = gated\n",
        "        \n",
        "        # create rgcn layers\n",
        "        self.build_model()\n",
        "       \n",
        "    def build_model(self):        \n",
        "        self.layers = nn.ModuleList() \n",
        "        for _ in range(self.num_hidden_layers):\n",
        "            rgcn_layer = RGCNLayer(self.h_dim, self.num_rels, activation=F.relu, gated = self.gated)\n",
        "            self.layers.append(rgcn_layer)\n",
        "    \n",
        "    def forward(self, g):\n",
        "        for layer in self.layers:\n",
        "            layer(g)\n",
        "        \n",
        "        rst_hidden = []\n",
        "        for sub_g in dgl.unbatch(g):\n",
        "            rst_hidden.append(  sub_g.ndata['h']   )\n",
        "        return rst_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XljUmRlRuIXN"
      },
      "source": [
        "class MyHead(nn.Module):\n",
        "    \"\"\"The MLP submodule\"\"\"\n",
        "    def __init__(self, gcn_out_size: int, bert_out_size: int):\n",
        "        super().__init__()\n",
        "        self.gcn_out_size = gcn_out_size\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.BatchNorm1d(gcn_out_size + bert_out_size),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(gcn_out_size + bert_out_size, 5),\n",
        "        )\n",
        "        for i, module in enumerate(self.fc):\n",
        "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
        "                nn.init.constant_(module.weight, 1)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.Linear):\n",
        "                if getattr(module, \"weight_v\", None) is not None:\n",
        "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
        "                    nn.init.kaiming_normal_(module.weight_v)\n",
        "                    assert model[i].weight_g is not None\n",
        "                else:\n",
        "                    nn.init.kaiming_normal_(module.weight)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "                \n",
        "    def forward(self, gcn_outputs , bert_embeddings):\n",
        "        \n",
        "        gcn_outputs = [ torch.mean(gcn_outputs[i],0).unsqueeze(0) for i in range(len(gcn_outputs)) ]\n",
        "        \n",
        "        gcn_extracted_outputs = [gcn_outputs[i].unsqueeze(0).view(gcn_outputs[i].unsqueeze(0).size(0), -1) for i in range(len(gcn_outputs))]\n",
        "        \n",
        "        gcn_extracted_outputs = torch.stack(gcn_extracted_outputs, dim=0).squeeze()\n",
        "        \n",
        "        embeddings = torch.cat((gcn_extracted_outputs, bert_embeddings), 1) \n",
        "        #print(gcn_extracted_outputs.shape)\n",
        "        \n",
        "        return self.fc(embeddings)\n",
        "\n",
        "class GPRModel(nn.Module):\n",
        "    \"\"\"The main model.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mBert = transformers.AutoModel.from_pretrained(BERT_MODEL)\n",
        "        self.RGCN =  RGCNModel(h_dim = 1024, num_rels = 3, num_hidden_layers = 1 , gated = True)\n",
        "        \n",
        "        self.head = MyHead(RGCN_output_size_1 , mBERT_OUTPUT_SIZE)\n",
        "    \n",
        "    def forward(self, g , ids, mask, token_type_ids):\n",
        "        gcn_outputs = self.RGCN(g)\n",
        "        \n",
        "        _, output_1 = self.mBert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "\n",
        "        head_outputs = self.head(gcn_outputs , output_1)\n",
        "        return head_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tAEODVISroO"
      },
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "val_dataset = GPRDataset(original_df = df_valid_hindi, graphs = all_graphs_valid, dataframe=new_df_valid , tokenizer=tokenizer,max_len=MAX_LEN)\n",
        "train_dataset = GPRDataset(original_df = df_train_hindi, graphs = all_graphs, dataframe=new_df , tokenizer=tokenizer,max_len=MAX_LEN)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "train_dataset,\n",
        "collate_fn = collate,\n",
        "batch_size = 4,\n",
        "shuffle=True,)\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "val_dataset,\n",
        "collate_fn = collate,\n",
        "batch_size = 4,)\n",
        "\n",
        "model = GPRModel().cuda()\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr_value)\n",
        "reg_lambda = 0.035"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc6Em1jq4jt9"
      },
      "source": [
        "df_test_hindi = pd.read_csv('Constraint_Hindi_Test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTpQot7rPiyV"
      },
      "source": [
        "df_test_hindi['Labels'] = fix_labels(df_test_hindi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xRVGKEBPrLQ"
      },
      "source": [
        "new_df_test = df_test_hindi[['Post', 'Labels']].copy()\r\n",
        "new_df_test.columns = ['comment_text' , 'list']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF_igcL3512L"
      },
      "source": [
        "token_lst_test = pickle.load(open('token_lst_wto_padding_test.pkl', \"rb\")) # tokens of every sentence without padding\n",
        "bert_outputs_lst_test = pickle.load(open('bert_outputs_test.pkl', \"rb\")) # list of outputs of bert for every sentence\n",
        "\n",
        "all_graphs_test = load_graphs(\"graph_data_test.bin\")[0]\n",
        "all_graphs_test = [i.to('cuda') for i in all_graphs_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uw-CfDMP3v6"
      },
      "source": [
        "test_dataset = GPRDataset(original_df = df_test_hindi, graphs = all_graphs_test, dataframe=new_df_test , tokenizer=tokenizer,max_len=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fLslapyQDA9"
      },
      "source": [
        "test_dataloader = DataLoader(\r\n",
        "    test_dataset,\r\n",
        "    collate_fn = collate,\r\n",
        "    batch_size = 3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ERKmZpARUd7",
        "outputId": "c2d10e12-df39-4b56-9fdf-61c93c7d43a2"
      },
      "source": [
        "if os.path.exists('best_model_state_dict.pth'):\n",
        "  print(\"loading the best model so far\")\n",
        "  model.load_state_dict(torch.load('best_model_state_dict.pth'))\n",
        "else:\n",
        "  print(\"No Pretrained Model Found !\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Pretrained Model Found !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqySNtMENcQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470fabbf-3c13-4bf1-fdac-2fbae5363dbb"
      },
      "source": [
        "\n",
        "\n",
        "best_val_f1_score = 0\n",
        "\n",
        "for epoch in tqdm.tqdm(range(total_epoch) , desc = 'Epoch: ' , position=0, leave=True):\n",
        "\n",
        "  training_loss = 0\n",
        "\n",
        "  #if epoch % 5 == 0:\n",
        "  #    print('|',\">\" * epoch,\" \"*(80-epoch),'|')\n",
        "\n",
        "  lr = adjust_learning_rate([optimizer],epoch)\n",
        "  #print(\"Learning rate = %4f\\n\" % lr)\n",
        "  model.train()\n",
        "\n",
        "  for iter, (batched_graph, labels , ids, mask, token_type_ids) in tqdm.tqdm(enumerate(train_dataloader),position=0, leave=True):\n",
        "\n",
        "      #bert_embeddings = bert_embeddings.cuda()\n",
        "      labels = labels.cuda()\n",
        "      \n",
        "      prediction = model( batched_graph , ids, mask, token_type_ids)\n",
        "      \n",
        "      l2_reg = None\n",
        "      for w in model.RGCN.parameters():\n",
        "          if not l2_reg:\n",
        "              l2_reg = w.norm(2)\n",
        "          else:\n",
        "              l2_reg = l2_reg + w.norm(2)  \n",
        "      for w in model.head.parameters():\n",
        "          if not l2_reg:\n",
        "              l2_reg = w.norm(2)\n",
        "          else:\n",
        "              l2_reg = l2_reg + w.norm(2)   \n",
        "      \"\"\"\n",
        "      for w in model.BERThead.parameters():\n",
        "          if not l2_reg:\n",
        "              l2_reg = w.norm(2)\n",
        "          else:\n",
        "              l2_reg = l2_reg + w.norm(2) \n",
        "      \"\"\"\n",
        "      loss = loss_func(prediction, labels) \n",
        "      #print(loss)\n",
        "        #+ l2_reg * reg_lambda\n",
        "      #loss = loss_func(prediction, labels)\n",
        "      training_loss += loss.detach().item()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  training_loss = training_loss / (iter+1)\n",
        "      \n",
        "  val_loss = 0\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  acts = []\n",
        "  with torch.no_grad():\n",
        "      for iter, (batched_graph, labels , ids, mask, token_type_ids) in tqdm.tqdm(enumerate(val_dataloader),position = 0):\n",
        "          #bert_embeddings = bert_embeddings.cuda()\n",
        "          labels = labels.cuda()\n",
        "          prediction = model(batched_graph , ids, mask, token_type_ids)\n",
        "          loss = loss_func(prediction, labels)\n",
        "          val_loss += loss.detach().item()\n",
        "          preds.append(prediction)\n",
        "          acts.append(labels)\n",
        "      val_loss = val_loss/(iter + 1)\n",
        "      \n",
        "      actual = np.vstack([j.cpu() for i in acts for j in i])\n",
        "      predi = np.vstack([np.array([1.0 if k > 0.5 else 0.0 for k in sigmoid_v(j.cpu())]) for i in preds for j in i])\n",
        "      \n",
        "  val_f1_score =  f1_score(y_true=actual, y_pred=predi, average=\"weighted\")\n",
        "  #if epoch%20 == 0:\n",
        "  print('Epoch {}, training_loss {:.4f} , val_loss {:.4f} , f1_score: {:.4f}'.format(epoch, training_loss , val_loss , val_f1_score))\n",
        "  print(metrics.classification_report(actual,predi))\n",
        "\n",
        "  test_loss = 0\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  acts = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for iter, (batched_graph , labels , ids, mask, token_type_ids) in tqdm.tqdm(enumerate(test_dataloader),position=0):\n",
        "          labels = labels.cuda()\n",
        "          prediction = model(batched_graph , ids, mask, token_type_ids)\n",
        "          loss = loss_func(prediction, labels)\n",
        "          test_loss += loss.detach().item()\n",
        "          preds.append(prediction)\n",
        "          acts.append(labels)\n",
        "      test_loss = test_loss/(iter + 1)\n",
        "      \n",
        "      actual = np.vstack([j.cpu() for i in acts for j in i])\n",
        "      predi = np.vstack([np.array([1.0 if k >= min(0.5,max(sigmoid_v(j.cpu()))) else 0.0 for k in sigmoid_v(j.cpu())]) for i in preds for j in i])\n",
        "      \n",
        "  test_f1_score =  f1_score(y_true=actual, y_pred=predi, average=\"weighted\")\n",
        "  #if epoch%20 == 0:\n",
        "  print('Epoch {}, training_loss {:.4f} , val_loss {:.4f} , f1_score: {:.4f}'.format(epoch, training_loss , test_loss , test_f1_score))\n",
        "  print(metrics.classification_report(actual,predi))\n",
        "\n",
        "\n",
        "  if test_f1_score > best_val_f1_score:\n",
        "      best_val_f1_score = test_f1_score\n",
        "      torch.save(model.state_dict(), 'best_model_state_dict.pth') \n",
        "      torch.save(model, 'best_model.pt') \n",
        "      #if epoch > 36: print('Best val loss found: ', best_val_loss)\n",
        "      \n",
        "  torch.cuda.empty_cache()\n",
        "  sleep(60)\n",
        "\n",
        "print('This fold, the best f1_score is: ', best_val_f1_score)\n",
        "#break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "1432it [05:05,  4.68it/s]\n",
            "203it [00:14, 14.27it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "3it [00:00, 21.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, training_loss 0.6627 , val_loss 0.4310 , f1_score: 0.7275\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.78      0.38        77\n",
            "           1       0.59      0.76      0.66       160\n",
            "           2       0.36      0.78      0.49       110\n",
            "           3       0.90      0.96      0.93       435\n",
            "           4       0.34      0.80      0.48       103\n",
            "\n",
            "   micro avg       0.55      0.87      0.68       885\n",
            "   macro avg       0.49      0.81      0.59       885\n",
            "weighted avg       0.66      0.87      0.73       885\n",
            " samples avg       0.68      0.88      0.74       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:29, 18.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, training_loss 0.6627 , val_loss 0.4336 , f1_score: 0.7272\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.68      0.35       169\n",
            "           1       0.62      0.76      0.69       334\n",
            "           2       0.38      0.74      0.50       234\n",
            "           3       0.91      0.96      0.93       873\n",
            "           4       0.37      0.82      0.51       219\n",
            "\n",
            "   micro avg       0.56      0.85      0.68      1829\n",
            "   macro avg       0.50      0.79      0.59      1829\n",
            "weighted avg       0.66      0.85      0.73      1829\n",
            " samples avg       0.69      0.87      0.74      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:05,  4.68it/s]\n",
            "203it [00:14, 14.42it/s]\n",
            "3it [00:00, 21.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, training_loss 0.5018 , val_loss 0.4492 , f1_score: 0.7295\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.73      0.36        77\n",
            "           1       0.53      0.85      0.65       160\n",
            "           2       0.35      0.87      0.50       110\n",
            "           3       0.97      0.91      0.94       435\n",
            "           4       0.35      0.77      0.48       103\n",
            "\n",
            "   micro avg       0.55      0.86      0.67       885\n",
            "   macro avg       0.49      0.83      0.59       885\n",
            "weighted avg       0.68      0.86      0.73       885\n",
            " samples avg       0.67      0.87      0.73       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:28, 19.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, training_loss 0.5018 , val_loss 0.4464 , f1_score: 0.7281\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.69      0.35       169\n",
            "           1       0.55      0.86      0.67       334\n",
            "           2       0.33      0.80      0.47       234\n",
            "           3       0.97      0.92      0.94       873\n",
            "           4       0.38      0.81      0.52       219\n",
            "\n",
            "   micro avg       0.55      0.86      0.67      1829\n",
            "   macro avg       0.50      0.81      0.59      1829\n",
            "weighted avg       0.67      0.86      0.73      1829\n",
            " samples avg       0.67      0.87      0.73      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:06,  4.67it/s]\n",
            "203it [00:14, 14.38it/s]\n",
            "3it [00:00, 21.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2, training_loss 0.4240 , val_loss 0.3669 , f1_score: 0.7637\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.58      0.40        77\n",
            "           1       0.67      0.84      0.75       160\n",
            "           2       0.39      0.82      0.53       110\n",
            "           3       0.96      0.94      0.95       435\n",
            "           4       0.41      0.69      0.52       103\n",
            "\n",
            "   micro avg       0.64      0.85      0.73       885\n",
            "   macro avg       0.55      0.77      0.63       885\n",
            "weighted avg       0.72      0.85      0.76       885\n",
            " samples avg       0.74      0.87      0.78       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:28, 19.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2, training_loss 0.4240 , val_loss 0.3668 , f1_score: 0.7596\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.53      0.39       169\n",
            "           1       0.68      0.81      0.74       334\n",
            "           2       0.38      0.73      0.50       234\n",
            "           3       0.96      0.95      0.96       873\n",
            "           4       0.45      0.76      0.57       219\n",
            "\n",
            "   micro avg       0.65      0.84      0.73      1829\n",
            "   macro avg       0.56      0.76      0.63      1829\n",
            "weighted avg       0.71      0.84      0.76      1829\n",
            " samples avg       0.75      0.86      0.78      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:08,  4.64it/s]\n",
            "203it [00:14, 14.26it/s]\n",
            "3it [00:00, 22.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3, training_loss 0.3602 , val_loss 0.3073 , f1_score: 0.7579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.17      0.22        77\n",
            "           1       0.81      0.73      0.77       160\n",
            "           2       0.48      0.60      0.53       110\n",
            "           3       0.97      0.95      0.96       435\n",
            "           4       0.47      0.59      0.53       103\n",
            "\n",
            "   micro avg       0.76      0.76      0.76       885\n",
            "   macro avg       0.61      0.61      0.60       885\n",
            "weighted avg       0.77      0.76      0.76       885\n",
            " samples avg       0.75      0.78      0.76       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:29, 18.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3, training_loss 0.3602 , val_loss 0.3050 , f1_score: 0.7727\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.24      0.31       169\n",
            "           1       0.78      0.78      0.78       334\n",
            "           2       0.49      0.64      0.56       234\n",
            "           3       0.96      0.97      0.97       873\n",
            "           4       0.49      0.70      0.58       219\n",
            "\n",
            "   micro avg       0.75      0.79      0.77      1829\n",
            "   macro avg       0.63      0.67      0.64      1829\n",
            "weighted avg       0.76      0.79      0.77      1829\n",
            " samples avg       0.80      0.83      0.80      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:07,  4.66it/s]\n",
            "203it [00:14, 14.28it/s]\n",
            "3it [00:00, 21.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4, training_loss 0.3089 , val_loss 0.2687 , f1_score: 0.7586\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.23      0.29        77\n",
            "           1       0.81      0.77      0.79       160\n",
            "           2       0.43      0.62      0.51       110\n",
            "           3       0.97      0.96      0.96       435\n",
            "           4       0.53      0.42      0.47       103\n",
            "\n",
            "   micro avg       0.77      0.76      0.76       885\n",
            "   macro avg       0.63      0.60      0.60       885\n",
            "weighted avg       0.77      0.76      0.76       885\n",
            " samples avg       0.76      0.79      0.77       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:28, 19.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4, training_loss 0.3089 , val_loss 0.2651 , f1_score: 0.7697\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.29      0.35       169\n",
            "           1       0.75      0.82      0.79       334\n",
            "           2       0.45      0.62      0.52       234\n",
            "           3       0.97      0.96      0.96       873\n",
            "           4       0.56      0.58      0.57       219\n",
            "\n",
            "   micro avg       0.76      0.78      0.77      1829\n",
            "   macro avg       0.63      0.65      0.64      1829\n",
            "weighted avg       0.76      0.78      0.77      1829\n",
            " samples avg       0.80      0.81      0.79      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:10,  4.61it/s]\n",
            "203it [00:14, 14.21it/s]\n",
            "3it [00:00, 22.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5, training_loss 0.2658 , val_loss 0.2402 , f1_score: 0.7668\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.27      0.30        77\n",
            "           1       0.80      0.81      0.80       160\n",
            "           2       0.50      0.51      0.50       110\n",
            "           3       0.96      0.97      0.97       435\n",
            "           4       0.58      0.44      0.50       103\n",
            "\n",
            "   micro avg       0.79      0.76      0.77       885\n",
            "   macro avg       0.63      0.60      0.61       885\n",
            "weighted avg       0.77      0.76      0.77       885\n",
            " samples avg       0.77      0.80      0.78       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:29, 18.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5, training_loss 0.2658 , val_loss 0.2390 , f1_score: 0.7795\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.33      0.36       169\n",
            "           1       0.76      0.84      0.80       334\n",
            "           2       0.54      0.56      0.55       234\n",
            "           3       0.95      0.98      0.97       873\n",
            "           4       0.59      0.56      0.57       219\n",
            "\n",
            "   micro avg       0.78      0.79      0.78      1829\n",
            "   macro avg       0.65      0.66      0.65      1829\n",
            "weighted avg       0.77      0.79      0.78      1829\n",
            " samples avg       0.81      0.82      0.81      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:09,  4.62it/s]\n",
            "203it [00:14, 14.26it/s]\n",
            "3it [00:00, 21.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6, training_loss 0.2203 , val_loss 0.2318 , f1_score: 0.7683\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.36      0.40        77\n",
            "           1       0.77      0.79      0.78       160\n",
            "           2       0.51      0.45      0.48       110\n",
            "           3       0.99      0.94      0.96       435\n",
            "           4       0.54      0.49      0.51       103\n",
            "\n",
            "   micro avg       0.80      0.75      0.77       885\n",
            "   macro avg       0.65      0.61      0.63       885\n",
            "weighted avg       0.79      0.75      0.77       885\n",
            " samples avg       0.76      0.78      0.76       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:29, 18.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6, training_loss 0.2203 , val_loss 0.2345 , f1_score: 0.7829\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.39      0.39       169\n",
            "           1       0.80      0.81      0.81       334\n",
            "           2       0.54      0.54      0.54       234\n",
            "           3       0.98      0.96      0.97       873\n",
            "           4       0.57      0.58      0.58       219\n",
            "\n",
            "   micro avg       0.78      0.78      0.78      1829\n",
            "   macro avg       0.66      0.66      0.66      1829\n",
            "weighted avg       0.79      0.78      0.78      1829\n",
            " samples avg       0.81      0.81      0.80      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:09,  4.63it/s]\n",
            "203it [00:14, 14.37it/s]\n",
            "3it [00:00, 22.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7, training_loss 0.1954 , val_loss 0.2325 , f1_score: 0.7664\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.34      0.36        77\n",
            "           1       0.77      0.78      0.77       160\n",
            "           2       0.54      0.39      0.46       110\n",
            "           3       0.99      0.94      0.96       435\n",
            "           4       0.52      0.58      0.55       103\n",
            "\n",
            "   micro avg       0.79      0.75      0.77       885\n",
            "   macro avg       0.64      0.61      0.62       885\n",
            "weighted avg       0.79      0.75      0.77       885\n",
            " samples avg       0.77      0.78      0.77       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:28, 19.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7, training_loss 0.1954 , val_loss 0.2337 , f1_score: 0.7861\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.41      0.42       169\n",
            "           1       0.79      0.84      0.82       334\n",
            "           2       0.60      0.44      0.50       234\n",
            "           3       0.98      0.96      0.97       873\n",
            "           4       0.53      0.68      0.60       219\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      1829\n",
            "   macro avg       0.66      0.67      0.66      1829\n",
            "weighted avg       0.79      0.79      0.79      1829\n",
            " samples avg       0.82      0.82      0.81      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1432it [05:08,  4.65it/s]\n",
            "203it [00:14, 14.40it/s]\n",
            "3it [00:00, 21.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8, training_loss 0.1746 , val_loss 0.2695 , f1_score: 0.7505\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.19      0.26        77\n",
            "           1       0.98      0.54      0.69       160\n",
            "           2       0.42      0.64      0.51       110\n",
            "           3       0.96      0.97      0.96       435\n",
            "           4       0.46      0.72      0.56       103\n",
            "\n",
            "   micro avg       0.75      0.75      0.75       885\n",
            "   macro avg       0.64      0.61      0.60       885\n",
            "weighted avg       0.79      0.75      0.75       885\n",
            " samples avg       0.75      0.77      0.75       885\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "551it [00:28, 19.20it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8, training_loss 0.1746 , val_loss 0.2742 , f1_score: 0.7587\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.29      0.36       169\n",
            "           1       0.95      0.56      0.71       334\n",
            "           2       0.47      0.63      0.54       234\n",
            "           3       0.95      0.98      0.96       873\n",
            "           4       0.46      0.71      0.56       219\n",
            "\n",
            "   micro avg       0.75      0.76      0.76      1829\n",
            "   macro avg       0.66      0.63      0.63      1829\n",
            "weighted avg       0.79      0.76      0.76      1829\n",
            " samples avg       0.79      0.79      0.78      1829\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1144it [04:06,  4.90it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY5zLFmR6Dp1"
      },
      "source": [
        "model.load_state_dict(torch.load('best_model_state_dict.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt2yE2OXf1H1"
      },
      "source": [
        "model.eval()\n",
        "preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for iter, (batched_graph , ids, mask, token_type_ids) in tqdm.tqdm(enumerate(test_dataloader),position=0):\n",
        "        prediction = model(batched_graph , ids, mask, token_type_ids)\n",
        "        preds.append(prediction)\n",
        "        \n",
        "    val_loss = val_loss/(iter + 1)\n",
        "    \n",
        "    actual = np.vstack([j.cpu() for i in acts for j in i])\n",
        "    predi = np.vstack([np.array([1.0 if k >= min(0.5,max(sigmoid_v(j.cpu()))) else 0.0 for k in sigmoid_v(j.cpu())]) for i in preds for j in i])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_TxaGGy83qx"
      },
      "source": [
        "final_preds = []\n",
        "\n",
        "for i in predi:\n",
        "  p = \"\"\n",
        "  for index , pred in enumerate(i):\n",
        "    if pred == 1:\n",
        "      p = p+cats[index]+\",\"\n",
        "  final_preds.append(p[:-1])\n",
        "\n",
        "df_test_hindi['Labels Set'] = final_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoR3H407-IOT"
      },
      "source": [
        "df_test_hindi.to_csv('test_preds.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}